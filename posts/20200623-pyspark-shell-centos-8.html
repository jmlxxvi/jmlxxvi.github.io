<article>
    <p>Posted on Tuesday 23 June 2020 </p>
    <div class="article_title">
        <h3>PySpark shell on Spark 3</h3>
    </div>

    <div class="article_text">

        <p>
            If you followed along this  
            <a class="internal_link" href="?20200622-install-centos-8-for-spark.html">post</a>
            you have a functional Spark server able to run the Scala shell.
            <br>
            But you may realize that as a soon as you try to start the Python shell you get:
        </p>

        <pre><code class="shell">[data@sp3m ~]$ pyspark
env: ‘python’: No such file or directory</code></pre>

        <p>
            This is because Python is not installed on the box.
            <br>
            To do that simply:
        </p>

        <pre><code class="shell">[data@sp3m ~]$ sudo yum install python38
Last metadata expiration check: 2:01:56 ago on Tue 23 Jun 2020 09:06:41 AM EDT.
Dependencies resolved.
=============================================================================================
    Package                Architecture  Version                              Repository  Size
=============================================================================================
Installing:
    python38                     x86_64  3.8.0-6.module_el8.2.0+317+61fa6e7d   AppStream  76 k
Installing dependencies:
    python38-libs                x86_64  3.8.0-6.module_el8.2.0+317+61fa6e7d   AppStream  8.2 M
    python38-pip-wheel           noarch  19.2.3-5.module_el8.2.0+317+61fa6e7d  AppStream  1.2 M
    python38-setuptools-wheel    noarch  41.6.0-4.module_el8.2.0+317+61fa6e7d  AppStream  304 k
Installing weak dependencies:
    python38-pip                 noarch  19.2.3-5.module_el8.2.0+317+61fa6e7d  AppStream  1.9 M
    python38-setuptools          noarch  41.6.0-4.module_el8.2.0+317+61fa6e7d  AppStream  667 k
Enabling module streams:
    python38                             3.8

Transaction Summary
=============================================================================================
Install  6 Packages

Total download size: 12 M
Installed size: 45 M
Is this ok [y/N]: y</code></pre>

    <p>
        After that you can check the installed version with
    </p>

    <pre><code class="shell">[data@sp3m ~]$ python3 --version
Python 3.8.0</code></pre>

        <p>
            The thing is that several tools (like Spark) expect the Python binary to be called just
            <span class="monospaced">python</span>
            and not
            <span class="monospaced">python3</span>
            <br>
            We are correcting that now:
        </p>

        <pre><code class="shell">[data@sp3m ~]$ sudo alternatives --set python /usr/bin/python3
[sudo] password for data: 
[data@sp3m ~]$ python --version
Python 3.8.0</code></pre>

        <p>
            Now any program needing Python have it available, including Spark
        </p>

        <pre><code class="shell">[data@sp3m ~]$ pyspark 
Python 3.8.0 (default, May  7 2020, 02:49:39) 
[GCC 8.3.1 20191121 (Red Hat 8.3.1-5)] on linux
Type "help", "copyright", "credits" or "license" for more information.
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/data/spark/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
20/06/23 13:22:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
       ____              __
      / __/__  ___ _____/ /__
     _\ \/ _ \/ _ `/ __/  '_/
    /__ / .__/\_,_/_/ /_/\_\   version 3.0.0
       /_/

Using Python version 3.8.0 (default, May  7 2020 02:49:39)
SparkSession available as 'spark'.
>>> 
</code></pre>

        <p>
            Here we go. Now just an example to make sure it works: 
        </p>

        <pre><code class="python">>>> import random
>>> def inside(p):
...     x, y = random.random(), random.random()
...     return x*x + y*y < 1
... 
>>> count = sc.parallelize(range(0, 1000000)).filter(inside).count()
>>> print("Pi is roughly %f" % (4.0 * count / 1000000))                         
Pi is roughly 3.142708
>>></code></pre>

        <p>
            That's pretty much it. Python is working on Spark!
        </p>
</article>


